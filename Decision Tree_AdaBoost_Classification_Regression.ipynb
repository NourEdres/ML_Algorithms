{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import random\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "pd.options.mode.chained_assignment = None\n",
    "def check_purity(data):\n",
    "    label_column = data[:, -1]\n",
    "    unique_classes = np.unique(label_column)\n",
    "    if len(unique_classes) == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def create_leaf(data, ml_task):\n",
    "    label_column = data[:, -1]\n",
    "    if ml_task == \"regression\":\n",
    "        leaf = np.mean(label_column)\n",
    "    # classfication\n",
    "    else:\n",
    "        unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n",
    "        index = counts_unique_classes.argmax()\n",
    "        leaf = unique_classes[index]\n",
    "    return leaf\n",
    "\n",
    "\n",
    "def get_potential_splits(data, random_subspace):\n",
    "    potential_splits = {}\n",
    "    _, n_columns = data.shape\n",
    "    column_indicis =list(range(n_columns-1))\n",
    "    if random_subspace and random_subspace <= len(column_indicis):\n",
    "        column_indicis = random.sample(population=column_indicis, k=random_subspace)\n",
    "\n",
    "    for column_index in column_indicis:  # excluding the last column which is the label\n",
    "        values = data[:, column_index]\n",
    "        unique_values = np.unique(values)\n",
    "        potential_splits[column_index] = unique_values\n",
    "\n",
    "    return potential_splits\n",
    "\n",
    "\n",
    "def split_data(data, split_column, split_value):\n",
    "    split_column_values = data[:, split_column]\n",
    "\n",
    "    type_of_feature = FEATURE_TYPES[split_column]\n",
    "    if type_of_feature == \"continuous\":\n",
    "        data_below = data[split_column_values <= split_value]\n",
    "        data_above = data[split_column_values > split_value]\n",
    "\n",
    "    # feature is categorical\n",
    "    else:\n",
    "        data_below = data[split_column_values == split_value]\n",
    "        data_above = data[split_column_values != split_value]\n",
    "\n",
    "    return data_below, data_above\n",
    "\n",
    "\n",
    "def calculate_mse(data):\n",
    "    actual_values = data[:, -1]\n",
    "    if len(actual_values) == 0:  # empty data\n",
    "        mse = 0\n",
    "    else:\n",
    "        prediction = np.mean(actual_values)\n",
    "        mse = np.mean((actual_values - prediction) ** 2)\n",
    "    return mse\n",
    "\n",
    "def calculate_overall_metric(data_below, data_above, metric_function):\n",
    "    n = len(data_below) + len(data_above)\n",
    "    p_data_below = len(data_below) / n\n",
    "    p_data_above = len(data_above) / n\n",
    "    overall_metric = (p_data_below * metric_function(data_below)\n",
    "                      + p_data_above * metric_function(data_above))\n",
    "    return overall_metric\n",
    "\n",
    "def class_counts(data):\n",
    "    counts = {}\n",
    "    for row in data:\n",
    "        label = row[-1]\n",
    "        if label not in counts:\n",
    "            counts[label] = 0\n",
    "        counts[label] += 1\n",
    "    return counts\n",
    "def gini(data):\n",
    "    counts = class_counts(data)\n",
    "    impurity = 1\n",
    "    for lbl in counts:\n",
    "        prob_of_lbl = counts[lbl] / float(len(data))\n",
    "        impurity -= prob_of_lbl**2\n",
    "    return impurity\n",
    "\n",
    "def information_gain(left, right, current_uncertainty):\n",
    "    p=float(len(left)) / (len(left)+len(right))\n",
    "    return current_uncertainty - p * gini(left) - (1 - p) * gini(right)\n",
    "\n",
    "def determine_best_split(data, potential_splits, ml_task):\n",
    "    first_iteration = True\n",
    "    for column_index in potential_splits:\n",
    "        for value in potential_splits[column_index]:\n",
    "            data_below, data_above = split_data(data, split_column=column_index, split_value=value)\n",
    "            if ml_task == \"regression\":\n",
    "                current_overall_metric = calculate_overall_metric(data_below, data_above, metric_function=calculate_mse)\n",
    "\n",
    "            # classification\n",
    "            else:\n",
    "                current_overall_metric = calculate_overall_metric(data_below, data_above,\n",
    "                                                                  metric_function=gini)\n",
    "\n",
    "            if first_iteration or current_overall_metric <= best_overall_metric:\n",
    "                first_iteration = False\n",
    "                best_overall_metric = current_overall_metric\n",
    "                best_split_column = column_index\n",
    "                best_split_value = value\n",
    "\n",
    "    return best_split_column, best_split_value\n",
    "\n",
    "\n",
    "def determine_type_of_feature(df):\n",
    "    feature_types = []\n",
    "    n_unique_values_treshold = 5\n",
    "    for feature in df.columns:\n",
    "        if feature != str(df.keys()[-1]):\n",
    "            unique_values = df[feature].unique()\n",
    "            example_value = unique_values[0]\n",
    "\n",
    "            if (isinstance(example_value, str)) or (len(unique_values) <= n_unique_values_treshold):\n",
    "                feature_types.append(\"categorical\")\n",
    "            else:\n",
    "                feature_types.append(\"continuous\")\n",
    "\n",
    "    return feature_types\n",
    "\n",
    "\n",
    "def decision_tree_algorithm(df, ml_task=\"classification\", counter=0, min_samples=2, max_depth=5, random_subspace=None):\n",
    "    # data preparations\n",
    "    if counter == 0:\n",
    "        global COLUMN_HEADERS, FEATURE_TYPES\n",
    "        COLUMN_HEADERS = df.columns\n",
    "        FEATURE_TYPES = determine_type_of_feature(df)\n",
    "        data = df.values\n",
    "    else:\n",
    "        data = df\n",
    "\n",
    "        # base cases\n",
    "    if (check_purity(data)) or (len(data) < min_samples) or (counter == max_depth):\n",
    "        leaf = create_leaf(data, ml_task)\n",
    "        return leaf\n",
    "    \n",
    "    # recursive part\n",
    "    else:\n",
    "        counter += 1\n",
    "        # helper functions\n",
    "        potential_splits = get_potential_splits(data,random_subspace)\n",
    "        split_column, split_value = determine_best_split(data, potential_splits, ml_task)\n",
    "        data_below, data_above = split_data(data, split_column, split_value)\n",
    "        # check for empty data\n",
    "        if len(data_below) == 0 or len(data_above) == 0:\n",
    "            leaf = create_leaf(data, ml_task)\n",
    "            return leaf\n",
    "        # determine question\n",
    "        feature_name = COLUMN_HEADERS[split_column]\n",
    "        type_of_feature = FEATURE_TYPES[split_column]\n",
    "        if type_of_feature == \"continuous\":\n",
    "            question = \"{} <= {}\".format(feature_name, split_value)\n",
    "        # feature is categorical\n",
    "        else:\n",
    "            question = \"{} = {}\".format(feature_name, split_value)\n",
    "        # instantiate sub-tree\n",
    "        sub_tree = {question: []}\n",
    "        # find answers (recursion)\n",
    "        yes_answer = decision_tree_algorithm(data_below, ml_task, counter, min_samples, max_depth,random_subspace)\n",
    "        no_answer = decision_tree_algorithm(data_above, ml_task, counter, min_samples, max_depth,random_subspace)\n",
    "        # If the answers are the same, then there is no point in asking the qestion.\n",
    "        # This could happen when the data is classified even though it is not pure\n",
    "        # yet (min_samples or max_depth base case).\n",
    "        if yes_answer == no_answer:\n",
    "            sub_tree = yes_answer\n",
    "        else:\n",
    "            sub_tree[question].append(yes_answer)\n",
    "            sub_tree[question].append(no_answer)\n",
    "\n",
    "        return sub_tree\n",
    "    \n",
    "def predict_example(example, tree):\n",
    "    if type(tree)==int or type(tree) == float or isinstance(tree,np.float64):\n",
    "        return tree\n",
    "    question = list(tree.keys())[0]\n",
    "    feature_name, comparison_operator, value = question.split(\" \")\n",
    "\n",
    "    # ask question\n",
    "    if comparison_operator == \"<=\":\n",
    "        if example[feature_name] <= float(value):\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "\n",
    "    # feature is categorical\n",
    "    else:\n",
    "        if str(example[feature_name]) == value:\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "\n",
    "    # base case\n",
    "    if not isinstance(answer, dict):\n",
    "        return answer\n",
    "\n",
    "    # recursive part\n",
    "    else:\n",
    "        residual_tree = answer\n",
    "        return predict_example(example, residual_tree)\n",
    "\n",
    "\n",
    "def calculate_accuracy(df, tree):\n",
    "    df[\"classification\"] = df.apply(predict_example, args=(tree,), axis=1)\n",
    "    df[\"classification_correct\"] = df[\"classification\"] == df.iloc[:,-1]\n",
    "    accuracy = df[\"classification_correct\"].mean()\n",
    "    return accuracy\n",
    "\n",
    "def score(predections,test):\n",
    "    predections_correct = predections==test.iloc[:,-1]\n",
    "    score = predections_correct.mean()\n",
    "    return score\n",
    "\n",
    "def grid_search(train_data,val_data,ml_task):\n",
    "    i=0\n",
    "    if ml_task==\"regression\":\n",
    "        grid_search = {\"max_depth\": [], \"min_samples\": [], \"MSE_train\": [], \"MSE_val\": []}\n",
    "        for max_depth in range(2, 10,2):\n",
    "            for min_samples in range(2 ,10 ,2):\n",
    "                tree = decision_tree_algorithm(train_data, ml_task=\"regression\", max_depth=max_depth,\n",
    "                                               min_samples=min_samples)\n",
    "\n",
    "                train_pred = decision_tree_predictions(train_data, tree)\n",
    "                MSE_train = metrics.mean_squared_error(train_data.iloc[:, -1], train_pred)\n",
    "                val_pred = decision_tree_predictions(val_data, tree)\n",
    "                MSE_val = metrics.mean_squared_error(val_data.iloc[:, -1], val_pred)\n",
    "\n",
    "                grid_search[\"max_depth\"].append(max_depth)\n",
    "                grid_search[\"min_samples\"].append(min_samples)\n",
    "                grid_search[\"MSE_train\"].append(MSE_train)\n",
    "                grid_search[\"MSE_val\"].append(MSE_val)\n",
    "                i+=1\n",
    "                print(f\"Progress: Iteration {i}/16\")\n",
    "        grid_search = pd.DataFrame(grid_search)\n",
    "        grid_search=grid_search.sort_values(by=\"MSE_val\", ascending=True,ignore_index=True)\n",
    "    else:\n",
    "        grid_search = {\"max_depth\": [], \"min_samples\": [], \"accurcy_train\": [], \"accurcy_val\": []}\n",
    "        for max_depth in range(2, 10,2):\n",
    "            for min_samples in range(2 ,10 ,2):\n",
    "                tree = decision_tree_algorithm(train_data,ml_task=\"classification\", max_depth=max_depth,\n",
    "                                               min_samples=min_samples)\n",
    "                train_pred=decision_tree_predictions(train_data,tree)\n",
    "                accurcy_train=score(train_pred,train_data)\n",
    "                val_pred = decision_tree_predictions(val_data, tree)\n",
    "                accurcy_val = score(val_pred, val_data)\n",
    "                grid_search[\"max_depth\"].append(max_depth)\n",
    "                grid_search[\"min_samples\"].append(min_samples)\n",
    "                grid_search[\"accurcy_train\"].append(accurcy_train)\n",
    "                grid_search[\"accurcy_val\"].append(accurcy_val)\n",
    "                i+=1\n",
    "                print(f\"Progress: Iteration {i}/16\")\n",
    "        grid_search = pd.DataFrame(grid_search)\n",
    "        grid_search=grid_search.sort_values(\"accurcy_val\", ascending=False,ignore_index=True)\n",
    "    return grid_search[\"max_depth\"][0] , grid_search[\"min_samples\"][0]\n",
    "\n",
    "def decision_tree_predictions(test,tree):\n",
    "    predictions = test.apply(predict_example, args=(tree,), axis=1)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_model_weight(error):\n",
    "    return 0.5*np.log((1-error)/(error+0.000001))\n",
    "def update_row_weights(row,alpha):\n",
    "    if row[-3] == row['y_pred']:\n",
    "        return row['weights'] * np.exp(-alpha)\n",
    "    else:\n",
    "        return row['weights'] * np.exp(alpha)\n",
    "def create_new_dataset(df):\n",
    "    indices = []\n",
    "    for i in range(df.shape[0]):\n",
    "        a = np.random.random()\n",
    "        indices.append(df.index[(df['cumsum_upper'] >= a) & (a > df['cumsum_lower'])].tolist()[0])\n",
    "    return indices\n",
    "def AdaboostAlgo(data,nEstimators):\n",
    "    df=data.copy()\n",
    "    size=df.shape[1]\n",
    "    MyModel=[]\n",
    "    for x in range(0,nEstimators):\n",
    "        df['weights'] = 1/df.shape[0]\n",
    "        stump=decision_tree_algorithm(df.iloc[:,:-1],max_depth=1)\n",
    "        df['y_pred'] = decision_tree_predictions(df.iloc[:,:-1],stump)\n",
    "        errorCells=df.loc[df['y_pred']!=df.iloc[:,-3]]\n",
    "        erorr=errorCells['weights'].sum()\n",
    "        modelW=calculate_model_weight(erorr)\n",
    "        df['updated_weights'] = df.apply(update_row_weights, args=(modelW,),axis=1)\n",
    "        df['nomalized_weights'] = df['updated_weights']/df['updated_weights'].sum()\n",
    "        df['cumsum_upper'] = np.cumsum(df['nomalized_weights'])\n",
    "        df['cumsum_lower'] = df['cumsum_upper'] - df['nomalized_weights']\n",
    "        index_values = create_new_dataset(df)\n",
    "        df=df.iloc[index_values,:size]\n",
    "        MyModel.append((stump,modelW))\n",
    "    return MyModel\n",
    "def AdaBoostPrediction(model,test):\n",
    "    predections=[]\n",
    "    sumPredetions=[0]*len(test)\n",
    "    for x in model:\n",
    "        predctions=decision_tree_predictions(test,x[0])\n",
    "        for i,k in enumerate(predctions):\n",
    "            if k == 1:\n",
    "                sumPredetions[i]+=x[1]\n",
    "            else:\n",
    "                sumPredetions[i]-=x[1]\n",
    "        predctions=[]\n",
    "    return [(w>0)*1 for w in sumPredetions]\n",
    "def ADBGridSearch(train,val):\n",
    "    i=0\n",
    "    grid_search = {\"n_estimator\": [],\"accurcy_val\": []}\n",
    "    for nE in [5,6,7,8,10,25]:\n",
    "        AdaBoostModel=AdaboostAlgo(train,nE)\n",
    "        y_predAdaBoost=AdaBoostPrediction(AdaBoostModel,val)\n",
    "        accurcy_val=score(y_predAdaBoost,val)\n",
    "        grid_search[\"n_estimator\"].append(nE)\n",
    "        grid_search[\"accurcy_val\"].append(accurcy_val)\n",
    "        i+=1\n",
    "        print(f\"Progress: Iteration {i}/10\")\n",
    "    grid_search = pd.DataFrame(grid_search)\n",
    "    grid_search=grid_search.sort_values(\"accurcy_val\", ascending=False,ignore_index=True)\n",
    "    print(grid_search)\n",
    "    return grid_search[\"n_estimator\"][0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification area Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B=1 and P=0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data-1.csv\", header=0)\n",
    "data[\"area_typeL\"]=(data[\"area_type\"]=='B')*1\n",
    "data=data.drop([\"area_type\",\"Unnamed: 0\"],axis='columns')\n",
    "data.columns=['availability','bedrooms',\"total_sqft\",\"bath\",\"balcony\",\"ranked\",\"price-in-rupees\",\"area_typeL\"]\n",
    "test, train, val=data[10051:], data[:8040], data[8041:10050]\n",
    "train_data=data[:10050]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: Iteration 1/16\n",
      "Progress: Iteration 2/16\n",
      "Progress: Iteration 3/16\n",
      "Progress: Iteration 4/16\n",
      "Progress: Iteration 5/16\n",
      "Progress: Iteration 6/16\n",
      "Progress: Iteration 7/16\n",
      "Progress: Iteration 8/16\n",
      "Progress: Iteration 9/16\n",
      "Progress: Iteration 10/16\n",
      "Progress: Iteration 11/16\n",
      "Progress: Iteration 12/16\n",
      "Progress: Iteration 13/16\n",
      "Progress: Iteration 14/16\n",
      "Progress: Iteration 15/16\n",
      "Progress: Iteration 16/16\n",
      "Grid Search Result: maxDipth=8 minSamples=2\n",
      "My tree accurcy is : 0.9060509554140127\n"
     ]
    }
   ],
   "source": [
    "max_d ,min_s=grid_search(train,val,ml_task=\"classification\")\n",
    "print(\"Grid Search Result: maxDipth=\"+str(max_d)+\" minSamples=\"+str(min_s))\n",
    "tree = decision_tree_algorithm(train_data,min_samples=min_s,max_depth=max_d)\n",
    "y_predDT=decision_tree_predictions(test,tree)\n",
    "accurcy=score(y_predDT,test)\n",
    "print(\"My tree accurcy is :\",accurcy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: Iteration 1/10\n",
      "Progress: Iteration 2/10\n",
      "Progress: Iteration 3/10\n",
      "Progress: Iteration 4/10\n",
      "Progress: Iteration 5/10\n",
      "Progress: Iteration 6/10\n",
      "   n_estimator  accurcy_val\n",
      "0            6     0.879542\n",
      "1            7     0.851170\n",
      "2           25     0.850174\n",
      "3            5     0.848183\n",
      "4            8     0.796416\n",
      "5           10     0.707815\n",
      "Grid Search Result: NEstimators=6\n"
     ]
    }
   ],
   "source": [
    "nEstimator=ADBGridSearch(train,val)\n",
    "print(\"Grid Search Result: NEstimators=\"+str(nEstimator))\n",
    "AdaBoostModel=AdaboostAlgo(train_data,nEstimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost accurcy is : 0.8586783439490446\n"
     ]
    }
   ],
   "source": [
    "y_predAdaBoost=AdaBoostPrediction(AdaBoostModel,test)\n",
    "accurcy=score(y_predAdaBoost,test)\n",
    "print(\"Adaboost accurcy is :\",accurcy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  sensitivity and specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = 0\n",
    "fn = 0\n",
    "tp = 0\n",
    "tn = 0\n",
    "\n",
    "for actual_value, predicted_value in zip(test.iloc[:,-1], y_predDT):\n",
    "    if predicted_value == actual_value:\n",
    "        if predicted_value == 1: \n",
    "            tp += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "    else: \n",
    "        if predicted_value == 1: \n",
    "            fp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "SensitivityDT=tp / (tp + fn)\n",
    "SpecificityDT=tn / (tn+fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = 0\n",
    "fn = 0\n",
    "tp = 0\n",
    "tn = 0\n",
    "\n",
    "for actual_value, predicted_value in zip(test.iloc[:,-1], y_predAdaBoost):\n",
    "    if predicted_value == actual_value:\n",
    "        if predicted_value == 1: \n",
    "            tp += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "    else: \n",
    "        if predicted_value == 1: \n",
    "            fp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "SensitivityAdaBoost=tp / (tp + fn)\n",
    "SpecificityAdaBoost=tn / (tn+fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree Sensitivity:  0.9514338575393154\n",
      "DecisionTree Specificity:  0.6257142857142857\n",
      "AdaBoost Sensitivity:  0.9875115633672525\n",
      "AdaBoost Specificity:  0.06285714285714286\n"
     ]
    }
   ],
   "source": [
    "print(\"DecisionTree Sensitivity: \",SensitivityDT)\n",
    "print(\"DecisionTree Specificity: \",SpecificityDT)\n",
    "print(\"AdaBoost Sensitivity: \",SensitivityAdaBoost)\n",
    "print(\"AdaBoost Specificity: \",SpecificityAdaBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"data-1.csv\", header=0)\n",
    "data1=data1.drop([\"Unnamed: 0\"],axis='columns')\n",
    "data1[\"area_type\"]=(data1[\"area_type\"]=='B')*1\n",
    "data1.columns=['area_type','availability','bedrooms',\"total_sqft\",\"bath\",\"balcony\",\"ranked\",\"price-in-rupees\"]\n",
    "test, train, val=data1[10051:], data1[:8040], data1[8041:10050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: Iteration 1/16\n",
      "Progress: Iteration 2/16\n",
      "Progress: Iteration 3/16\n",
      "Progress: Iteration 4/16\n",
      "Progress: Iteration 5/16\n",
      "Progress: Iteration 6/16\n",
      "Progress: Iteration 7/16\n",
      "Progress: Iteration 8/16\n",
      "Progress: Iteration 9/16\n",
      "Progress: Iteration 10/16\n",
      "Progress: Iteration 11/16\n",
      "Progress: Iteration 12/16\n",
      "Progress: Iteration 13/16\n",
      "Progress: Iteration 14/16\n",
      "Progress: Iteration 15/16\n",
      "Progress: Iteration 16/16\n",
      "Grid Search Result: maxDipth=6 minSamples=2\n",
      "My tree MSE is : 65230071096394.2\n"
     ]
    }
   ],
   "source": [
    "max_d ,min_s=grid_search(train,val,ml_task=\"regression\")\n",
    "print(\"Grid Search Result: maxDipth=\"+str(max_d)+\" minSamples=\"+str(min_s))\n",
    "train_data=data1[:10050]\n",
    "tree = decision_tree_algorithm(train_data,ml_task='regression',min_samples=min_s,max_depth=max_d)\n",
    "y_pred=decision_tree_predictions(test,tree)\n",
    "mse=metrics.mean_squared_error(test.iloc[:,-1],y_pred)\n",
    "print(\"My tree MSE is :\",mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
